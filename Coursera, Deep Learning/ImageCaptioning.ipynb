{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import download_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_utils.link_all_keras_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "L = keras.layers\n",
    "K = keras.backend\n",
    "\n",
    "import utils\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "from random import choice\n",
    "import grading_utils\n",
    "import os\n",
    "from keras_utils import reset_tf_session\n",
    "import tqdm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image features\n",
    "\n",
    "using a pre-trained InceptiionV3 model for CNN encoder\n",
    "(https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html) and extract its last hidden layer as an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take the last hidden layer of InceptionV# as an image embedding \n",
    "\n",
    "def get_cnn_encoder:\n",
    "    K.set_learning_phase(False)\n",
    "    model = keras.applications.InceptionV3(include_top=False)\n",
    "    preprocess_for_model = keras.applications.inception_v3.preprocess_input\n",
    "    \n",
    "    model = keras.models.Model(model.inputs, keras.layyers.GlobalAveragge\n",
    "                              pooling2D()(model.output))\n",
    "    return model, preprocess_for_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load pre trained model\n",
    "reset_tf_session()\n",
    "encoder, preprocess_for_model = get_cnn_encoder()\n",
    "\n",
    "##### extract train features\n",
    "\n",
    "train_img_embeds, train_img_fns = utils.apply_model(\n",
    " \"train2014.zip\", encoder, preprocess_for_model, input_shape=(IMG_SIZE,IMG_SIZE))\n",
    "utils.save_pickle(train_img_embeds, \"train_img_embeds.pickle\")\n",
    "utils.save_pickle(train_img_fns, \"train_img_fns.pickle\")\n",
    "\n",
    "##### extract validation features\n",
    "val_img_embs, val_img_fns = utils.apply_model(\n",
    "  \"val2014.zip\", encoder, preprocess_for_model, input_shape=(IMG_SIZE,IMG_SIZE))\n",
    "utils.save_pickle(val_img_embeds, \"val_img_embeds.pickle\")\n",
    "utils.save_pickle(val_img_fns, \" val_img_fns.pickle\")\n",
    "\n",
    "##### sample images for learners\n",
    "def sample_zip(fn_in, fn_out, rate = 0.01, seed= 42):\n",
    "    np.random.seed(seed)\n",
    "    with zipfile.ZipFile(fn_in) as fin, zipfiile.ZipFile(fn_out, 'w') as fout:\n",
    "        sampled = filter(lamda_:np.random.rand()< rate, fin.filelist)\n",
    "        for zInfo in sampled:\n",
    "            fout.writestr(/zInfo,fin.read(zInfo))\n",
    "            \n",
    "sample_zip('train2014.zip', 'train2014_sample.zip')\n",
    "sample_zip('val2014.zip', 'val2014_sample.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prpared embeddings\n",
    "train_img_embeds = utils.read_pickle(\"train_img_embeds.pickle\")\n",
    "train_img_fns = utils.read_pickle(\"train_img_fns.pickle\")\n",
    "val_img_embeds = utils.read_pickle(\"val_img_embeds.pickle\")\n",
    "val_img_fns = utils.read_pickle(\"val_img_fns.pickle\")\n",
    "# check shapes\n",
    "print(train_img_embed.shape, len(train_img_fns))\n",
    "print(val_img_embeds.shape, len(val_img_fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check prepared samples of images\n",
    "\n",
    "list(filter(lambda x: x.endswith(\"_sample.zip\"), os.listdir(\".\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract captions for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract captions from zip\n",
    "\n",
    "def get_captions_for_fns( fns,zip_fn,zip_json_path):\n",
    "    zf=zipfile.Zipfile(zip_fn)\n",
    "    j= json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
    "    id_to_fn ={img['id']:img['file_name'] for img in j['images']}\n",
    "    fn_to_caps= defaultdict(list)\n",
    "    for cap in j['annotations']:\n",
    "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
    "    fn_to_caps = dict(fn_to_caps)\n",
    "    return list(map(lambda x: fn_to_caps[x], fns))\n",
    "\n",
    "train_captions = get_captions_for_fns(train_img_fns,\"captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_train2014.json\")\n",
    "val_captions = get_captions_for_fns(val_img_fns, \"captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_val2014.json\")\n",
    "\n",
    "#check shape\n",
    "print(len(train_img_fns), len(train_captions))\n",
    "print(len(val_img_fns), len(val_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at training example (each has 5 captions)\n",
    "\n",
    "def show_training_example(train_img_fns, train_captions, example_idx=0):\n",
    "    \"\"\"\n",
    "    you can changeexample_idx and see different images\n",
    "    \n",
    "    \"\"\"\n",
    "    zf = zipfile.ZipFile(\"train2014_sample.zip\")\n",
    "    captions_by_file = dict(zip(train_img_fns, train_captions))\n",
    "    all_files = set(train_img_fns)\n",
    "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all files, sf.filelist))\n",
    "    example = found_files[example_idx]\n",
    "    img = utils.decode_image_from_buf(zf.read(example))\n",
    "    plt.imshow(utils.image_center_crop(img))\n",
    "    plt.title(\"\\n\".join(captions_by_file[example.filename.rsplit(\"/\")[-1]]))\n",
    "    plt.show()\n",
    "    \n",
    "show_training_example(train_img_fns, train_captions, example_idx= 142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare captions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview captions data\n",
    "train_captions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    " \n",
    "\n",
    "# split sentence into tokens (split into lowercased words)\n",
    "def split_sentence(sentence):\n",
    "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
    "\n",
    "def generate_vocabulary(train_captions):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
    "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    for cap in train_captions:\n",
    "        for sentence in cap:\n",
    "            tokens=split_sentence(sentence)\n",
    "\n",
    "        for idx in tokens:\n",
    "          vocab_dict[idx]= vocab_dict.get(idx,0)+1\n",
    "    \n",
    "    vocab =[i for i,j in vocab_dict.items() if j>=5]\n",
    "    vocab += [PAD, UNK, START, END]\n",
    "\n",
    "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
    "    \n",
    "def caption_tokens_to_indices(captions, vocab):\n",
    "    \"\"\"\n",
    "    `captions` argument is an array of arrays:\n",
    "    [\n",
    "        [\n",
    "            \"image1 caption1\",\n",
    "            \"image1 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        [\n",
    "            \"image2 caption1\",\n",
    "            \"image2 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
    "    Add START and END tokens to start and end of each sentence respectively.\n",
    "    For the example above you should produce the following:\n",
    "    [\n",
    "        [\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    res=[]\n",
    "    for res in captions:\n",
    "        for sentence in res:\n",
    "          res1 = [vocab[START]]\n",
    "          res1 += [vocab[i] if i in vocab else vocab[UNK] for i in split_sentence(sentence)]\n",
    "          res1.append(vocab[END])\n",
    "        res.append(res1) ### YOUR CODE HERE ###\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vocabulary\n",
    "vocab = generate_vocabulary(train_captions)\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace tokens with indices\n",
    "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
    "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions have different length, but we need to batch them, that's why we will add PAD tokens so that all sentences have an equal length. \n",
    "\n",
    "We will crunch LSTM through all the tokens, but we will ignore padding tokens during loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this during training\n",
    "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
    "    \"\"\"\n",
    "    `batch_captions` is an array of arrays:\n",
    "    [\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        ...\n",
    "    ]\n",
    "    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),\n",
    "        where \"columns\" is max(map(len, batch_captions)) when max_len is None\n",
    "        and \"columns\" = min(max_len, max(map(len, batch_captions))) otherwise.\n",
    "    Add padding with pad_idx where necessary.\n",
    "    Input example: [[1, 2, 3], [4, 5]]\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
    "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
    "    Try to use numpy, we need this function to be fast!\n",
    "    \"\"\"\n",
    "    if (max_len==None):\n",
    "      columns = max(map(len, batch_captions))\n",
    "    else:\n",
    "      columns = min(max_len,max(map(len, batch_captions)))\n",
    "    \n",
    "    matrix= np.full((len(batch_captions), columns), fill_value= pad_idx)\n",
    "\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
